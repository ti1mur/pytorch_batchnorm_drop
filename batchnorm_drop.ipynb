{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133dc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import transforms, utils, datasets\n",
    "\n",
    "sns.set(font_scale=1.4, style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f93e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc7a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\"plane\", \"car\", \"bird\", \"cat\",\n",
    "           \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n",
    "\n",
    "def get_loaders(batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    trainset = datasets.CIFAR10(root=\"./data\", train=True,\n",
    "                               download=True, transform=transform)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=2)\n",
    "    testset = datasets.CIFAR10(root=\"./data\", train=False,\n",
    "                               download=False, transform=transform)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=2)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4afcae4",
   "metadata": {},
   "source": [
    "В PyTorch датасетом считается любой объект, для которого определены методы `__len__(self)` и `__getitem__(self, i)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad512d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, optim, train_dl, valid_dl):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "        for xb, yb in tqdm(train_dl):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            \n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss_sum += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "        train_losses.append(loss_sum / len(train_dl))\n",
    "    \n",
    "        model.eval()\n",
    "        loss_sum = 0\n",
    "        correct = 0\n",
    "        num = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in tqdm(valid_dl):\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "                probs = model(xb)\n",
    "                loss_sum += loss_func(probs, yb).item()\n",
    "\n",
    "                _, preds = torch.max(probs, axis=-1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                num += len(xb)\n",
    "        \n",
    "        val_losses.append(loss_sum / len(valid_dl))\n",
    "        valid_accuracies.append(correct / num)\n",
    "    \n",
    "    return train_losses, val_losses, valid_accuracies\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff86ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(train_losses, valid_losses, valid_accuracies):\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.plot(train_losses, label=\"train_losses\")\n",
    "    plt.plot(valid_losses, label=\"valid_losses\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.plot(valid_accuracies, label=\"valid_accuracies\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e205ef18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120) \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b918d493a2b44d1b8ff00cd3be60d7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "info = fit(10, model, criterion, optimizer, *get_loaders(4))\n",
    "plot_training(*info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa4832d",
   "metadata": {},
   "source": [
    "# BatchNorm\n",
    "Ранее мы обсуждали, что для линейных моделей очень важно нормировать признаки перед подачей на вход. Когда мы работаем с нейронными сетями мы тоже нормируем вход, но есть ли какой-то способ нормировать признаки на внутренних слоях нейроной сети?\n",
    "\n",
    "Да, существует несколько метдов нормировки признаков (BatchNorm, LayerNorm, InstanceNorm, etc). Научимся применять BatchNorm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc38a1e",
   "metadata": {},
   "source": [
    "### BatchNorm, математика\n",
    "\n",
    "Если коротко, то BatchNorm для каждого признака вычитает среднее значение по батчу и делит на стандартное отклонение по батчу, потом домножает все признаки на вес $\\gamma$ и прибавляет вес $\\beta$. При этом возникает вопрос, что если мы используем модель уже для предсказаний и можем запускать ее только на одном примере.\n",
    "\n",
    "BatchNorm работает по разному во время обучения и предсказний:\n",
    "\n",
    "\n",
    "**Во время обучения**. Пусть батч состоит из $\\mathbf{x_i}$ (каждый $\\mathbf{x_i}$ - вектор, подающийся на вход). Тогда\n",
    "$$\\begin{aligned}\n",
    "\\mu_{\\mathcal{B}} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\\n",
    "\\sigma_{\\mathcal{B}}^{2} & \\leftarrow \\frac{1}{m} \\sum_{i=1}^{m}\\left(x_{i}-\\mu_{\\mathcal{B}}\\right)^{2} \\\\\n",
    "\\widehat{x}_{i} & \\leftarrow \\frac{x_{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2}+\\epsilon}} \\\\\n",
    "y_{i} & \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{B} \\mathrm{N}_{\\gamma, \\beta}\\left(x_{i}\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Во время предсказания**. Мы делаем то же самое, но у нас нет батча. Поэтому в качестве $\\mu_{\\mathcal{B}}$ и $\\sigma_{\\mathcal{B}}$ мы используем среднее и стандартное отклонение признака во всем датасете. Обычно нам не хочется после обучения еще раз применять сеть ко всем примерам из обучающего датасета, чтобы вычислить эти статистики и мы вместо них используем экспоненциально затухающее среднее последних батчей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4826f",
   "metadata": {},
   "source": [
    "### BatchNorm, что он дает?\n",
    "\n",
    "* Более быстрое обучение. Болшие learning_rate\"ы.\n",
    "* Обучение более глубоких сетей.\n",
    "* Регуляризация.\n",
    "* Повышение точности моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b4eb8",
   "metadata": {},
   "source": [
    "### BatchNorm для Conv слоев\n",
    "\n",
    "Для сверточных слоев мы хотим следующее свойство \"если в разных частях картинки находятся одинаковые наборы пикселей, то соответствующие выходы сверточного слоя будут одинаковыми\". Если бы мы применяли алгоритм, который описан выше, то получилось бы так, что для пикселей, находящихся в 1 канале в координате (1,1) среднее и стд могли бы получиться не такими же как для пикселя в 1 канале в координате (10, 10). Тогда даже если изначально в них были одинаковые значения, то после BatchNorm они стали бы разными. \n",
    "\n",
    "Есть простое решение проблемы. Мы будем усреднять не только по batch_size координате, но и height, width координатам. Чтобы лучше объяснить используем псевдокод (origin https://stackoverflow.com/questions/38553927/batch-normalization-in-convolutional-neural-network):\n",
    "\n",
    "На вход подается тензор (многомерный массив) размера [B, H, W, C]. Где B - количество батчей, H - высота картинок, W - ширина картинок, а C - количество каналов. Тогда обычный батчнорм выполнял бы нормирование так:\n",
    "```python\n",
    "# t is the incoming tensor of shape [B, H, W, C]\n",
    "# mean and stddev are computed along 0 axis and have shape [H, W, C]\n",
    "mean = mean(t, axis=0)\n",
    "stddev = stddev(t, axis=0)\n",
    "for i in 0..B-1:\n",
    "  out[i,:,:,:] = norm(t[i,:,:,:], mean, stddev)\n",
    "```\n",
    "\n",
    "В то время как батчнорм для сверточных сетей (BatchNorm2D в PyTorch):\n",
    "\n",
    "```python\n",
    "# t is still the incoming tensor of shape [B, H, W, C]\n",
    "# but mean and stddev are computed along (0, 1, 2) axes and have just [C] shape\n",
    "mean = mean(t, axis=(0, 1, 2))\n",
    "stddev = stddev(t, axis=(0, 1, 2))\n",
    "for i in 0..B-1, x in 0..H-1, y in 0..W-1:\n",
    "  out[i,x,y,:] = norm(t[i,x,y,:], mean, stddev)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a801e",
   "metadata": {},
   "source": [
    "### BatchNorm, порядок применения\n",
    "\n",
    "В оригинальной статье (http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf) для сверточных слоев батчнорм предлагают использовать сразу после свертки до активации. Я не смог найти статей, которые бы исследовали, нужно ли делать BN до или после активации, и, похоже, однозначного мнения нет + в более сложных архитектурах (ResNet\"ы) исследователи обычно экспериментируют и ставят BN в разные места."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc475e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelBatchNorm, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(6) # кол -во каналов\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 5x5 image dimension\n",
    "        self.bn3 = nn.BatchNorm1d(120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.max_pool2d(F.relu(self.conv1(x)), (2, 2)))\n",
    "        x = self.bn2(F.max_pool2d(F.relu(self.conv2(x)), 2))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.bn3(F.relu(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a44d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "odel = ModelBatchNorm().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "info = fit(10, model, criterion, optimizer, *get_loaders(4))\n",
    "plot_training(*info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89319afb",
   "metadata": {},
   "source": [
    "Вспомните как выглядели формулы для BatchNorm: если размер батча равен 1, то $\\sigma = 0$, и у нас есть деление на ноль. PyTorch не умеет обрабатывать такой случай и падает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelBatchNorm().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-4, momentum=0.9)\n",
    "\n",
    "fit(1, model, criterion, optimizer, *get_loaders(batch_size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731cd79",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Дропаут это еще один необычный слой, который используется в нейронных сетях. У него есть один гиперпараметр $p$.\n",
    "\n",
    "Идея дропаута состоит в том, что во время обучения мы зануляем случайную часть входа и отдаем вход дальше (для каждого числа мы подбрасываем монетку и с вероятностью $p$ зануляем это число). \n",
    "\n",
    "Дропаут позволяет тренировать более устойчивые сети и избегать переобучения. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5ba27",
   "metadata": {},
   "source": [
    "### Dropout, механика работы.\n",
    "\n",
    "Как мы сказали выше, dropout зануляет случайную часть входов и отдает их дальше. Допустим $p=0.5$ (достаточное популярное значение). Тогда мы просто убираем половину всего входа! Такое сильное воздействие явно плохо повлияет на качество нашей модели, поэтому мы делаем зануление только во время обучения.\n",
    "\n",
    "**Во время обучения**: для каждого числа во входе подбрасываем монетку и зануляем его с вероятностью $p$. Выход умножаем на $\\frac{1}{1-p}$, чтобы дисперсия выходов осталось такой же, как и на входе.\n",
    "\n",
    "**Во время предсказаний**: ничего не делаем)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d338f12",
   "metadata": {},
   "source": [
    "### Dropout, что дает?\n",
    "\n",
    "* Сеть выучивает более устойчивые представления на внутренних слоях.\n",
    "* Сильно увеличивает число итераций, которые нужны для сходимости.\n",
    "* Можно получить интерпретацию, которая говорит, что дропаут усредняет выходы большо числа нейросетей с $p|W|$ нейронами на предыдущем слоев. \n",
    "\n",
    "Дропаут вызывает интересный эффект: в начале обучения качество на тестовом датасете выше, чем на обучающем. Потому что для обучающего датасета у нас есть зануление, которое сильно портит предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f78e4",
   "metadata": {},
   "source": [
    "### Dropout, взаимодействие с BatchNorm.\n",
    "\n",
    "Статья, исследующая, почему исопльзование дропаута и батчнорма вместе часто ведет к более плохим результатам, чем их использование по-отдельности - https://arxiv.org/pdf/1801.05134.pdf\n",
    "\n",
    "Картинка из статьи, объясняющая проблему:\n",
    "![img](https://media.arxiv-vanity.com/render-output/3934414/x1.png)\n",
    "\n",
    "(Если совсем коротко, то при наличии дропаута во время обучения и во время предсказаний выходы дропаута имеют разное распределение. Поэтому статистики, которые batchnorm считает для применения во время предсказаний, оказываются неверными.)\n",
    "\n",
    "Решение: если вы хотите использовать батчнорм и дропаут в одной сети, то все Dropout\"ы должны идти после BatchNorm\"ов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52dcd1",
   "metadata": {},
   "source": [
    "### Dropout, PyTorch\n",
    "\n",
    "В PyTorch есть `F.dropout(x, p=p)` и слой `nn.Dropout(p=p)`. В чем их отличие? `F.dropout(x, p=p)` не будет изменять свое поведение в заивимости от того, в каком стостянии сейчас модель (train, eval).\n",
    "\n",
    "Теперь чуть подробнее:\n",
    "\n",
    "Когда вы вызываете model.train()/model.eval() PyTorch проходится по всем переменным класса и если видит там наследника nn.Module или nn.ModuleList, то также меняет состояние для всех найденных модулей. Т.е. версия со слоем будет автоматически работать с train/eval состяниями. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelDropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelDropout().to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))\n",
    "\n",
    "info = fit(10, model, loss_func, optim, *get_loaders(4))\n",
    "plot_training(*info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fec60e",
   "metadata": {},
   "source": [
    "# Weight Decay\n",
    "\n",
    "Для регуляризации линейных моделей мы прибавляли к лоссу сумму квадратов весов, умноженных на некоторый коэффициент:\n",
    "$$L(\\mathbf{w})=\\sum_{i=1}^{l}\\left(\\mathbf{x}_{i}^{T} \\mathbf{w}-y_{i}\\right)^{2}+\\beta \\sum_{j=1}^{n} w_{j}^{2}$$\n",
    "\n",
    "Для нейронных сетей мы можем выбрать такую же реугляризацию. Она называется WeightDecay. Во многие оптимизаторы можно передать параметр `weight_decay` и он будет являться коэффициентом, на который домножается сумма квадратовв весов.\n",
    "\n",
    "Обычно используют weight_decay=0.01 или 0.005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelBatchNorm().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "info = fit(10, model, criterion, optimizer, *get_loaders(4))\n",
    "plot_training(*info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a8323",
   "metadata": {},
   "source": [
    "# LR scheduling\n",
    "\n",
    "Часто мы хотим, чтобы наш learning rate как-то изменялся во время обучения. Стратегия, по которой мы будем изменять lr называется lr scheduilng.\n",
    "\n",
    "Например, мы можем хотеть, чтобы learning_rate уменьшался с каждой эпохой в фиксированное число раз. Тогда в начале мы будем быстро двигаться к минимуму, а в конце точно не промахнемся мимо него за счет малых шагов. Такая стратегия называется LR Decay.\n",
    "![](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/images/lr2.png)\n",
    "\n",
    "(Слева используется lr decay, справа нет. Слева мы можем не подбирать идеально точно lr и все равно со временем сойтись.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl, lr_sched=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    valid_accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_sum = 0\n",
    "        for xb, yb in tqdm(train_dl):\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss_sum += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        train_losses.append(loss_sum / len(train_dl))\n",
    "\n",
    "        model.eval()\n",
    "        loss_sum = 0\n",
    "        correct = 0\n",
    "        num = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_dl:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                probs = model(xb)\n",
    "                loss_sum += loss_func(probs, yb).item()\n",
    "                \n",
    "                _, preds = torch.max(probs, axis=-1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                num += len(xb)\n",
    "                \n",
    "        val_losses.append(loss_sum / len(valid_dl))\n",
    "        valid_accuracies.append(correct / num)\n",
    "        \n",
    "        # CHANGES HERE\n",
    "        lr_sched.step()\n",
    "        # CHANGES END\n",
    "        \n",
    "    return train_losses, val_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelDropout().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=0.5)\n",
    "\n",
    "info = fit(10, model, criterion, optimizer, *get_loaders(4), lr_sched=scheduler)\n",
    "plot_training(*info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
